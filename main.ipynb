{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import itertools\n",
    "import json\n",
    "import time\n",
    "import operator\n",
    "EPSILON = 0.001\n",
    "DATASET_PATH = 'dataset/hw3dataset/'\n",
    "IBM_PATH = 'ibm/converted/'\n",
    "RESULT_PATH = 'result/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     13,
     41,
     53,
     66,
     83
    ]
   },
   "outputs": [],
   "source": [
    "class HITS:\n",
    "    def __init__(self, nodes, graph, iteration=20, delta=EPSILON):\n",
    "        self.authority = dict.fromkeys(graph, 1.0)\n",
    "        self.hubness = dict.fromkeys(graph, 1.0)\n",
    "        self.hubness_old = dict.fromkeys(graph, 1.0)\n",
    "        self.graph = graph.copy()\n",
    "        self.nodes = nodes.copy()\n",
    "        self.parent_nodes, self.children_nodes = self.collect_in_out_nodes()\n",
    "        self.delta = delta\n",
    "        self.iteration = iteration\n",
    "        self.find_parent()\n",
    "        \n",
    "    # main function to run all actions    \n",
    "    def run(self):\n",
    "        # iteration is needed to stable\n",
    "        for _iter in range(self.iteration):\n",
    "            for node in self.nodes:\n",
    "                self.authority[node] = self.find_authorities(node)\n",
    "                \n",
    "            self.authority = self.normalization(self.authority)\n",
    "            \n",
    "            for node in self.nodes:\n",
    "                self.hubness[node] = self.find_hubness(node)\n",
    "                \n",
    "            # normalization \n",
    "            self.hubness = self.normalization(self.hubness)\n",
    "            \n",
    "            # delta = absolute difference of the current hubness values and the last-time hubness values\n",
    "            # delta is checked whether is smaller then epsilon\n",
    "            delta_now = 0\n",
    "            for i in self.hubness:\n",
    "                delta_now += abs(self.hubness[i] - self.hubness_old[i])\n",
    "            if self.delta >= delta_now:\n",
    "                return self.authority, self.hubness\n",
    "\n",
    "            self.hubness_old = self.hubness.copy()\n",
    "            \n",
    "        return self.authority, self.hubness\n",
    "            \n",
    "    # find the authority of the node\n",
    "    # find out the sum of hubnedd of its parent\n",
    "    def find_authorities(self, node):\n",
    "        sum_of_hubness = 0.0\n",
    "        if node not in self.parent_:\n",
    "            return 0.0\n",
    "        \n",
    "        for parent in self.parent_[node]:\n",
    "            if parent != 0:\n",
    "                sum_of_hubness += self.hubness[parent]\n",
    "        return sum_of_hubness\n",
    "            \n",
    "    # find the hubness of the node\n",
    "    # find out the sum of authority of its children\n",
    "    def find_hubness(self, node):\n",
    "        sum_of_authorities = 0.0\n",
    "        if node not in self.graph:\n",
    "            return 0.0 \n",
    "\n",
    "        for child in self.graph[node]:\n",
    "            # sum of authority of children\n",
    "            if child != '0':\n",
    "                sum_of_authorities +=  self.authority[child]\n",
    "        return sum_of_authorities\n",
    "                \n",
    "    # to collect whether the node is parent of children of other nodes\n",
    "    # just for jugdement of finding parent and children            \n",
    "    def collect_in_out_nodes(self):\n",
    "        parent_nodes = []\n",
    "        children_nodes = []\n",
    "        for parent in self.graph:\n",
    "            n = self.graph[parent]\n",
    "\n",
    "            if any(x is '0' for x in self.graph[parent]) is False:\n",
    "                parent_nodes.append(parent)\n",
    "            \n",
    "            for child in self.graph[parent]:\n",
    "                if int(child) != 0:\n",
    "                    if child not in children_nodes:\n",
    "                        children_nodes.append(child)\n",
    "        \n",
    "        return parent_nodes, children_nodes\n",
    "    \n",
    "    # normalization of all the values after each itertion\n",
    "    def normalization(self, dic):\n",
    "        sum_all = 0.0\n",
    "        for item in dic:\n",
    "            sum_all += (dic[item])\n",
    "            \n",
    "        return {index: value / sum_all for (index, value) in dic.items()}\n",
    "    \n",
    "    def incease_authority(self, number = 1):\n",
    "        # to increase authority\n",
    "        # increase parent\n",
    "        # effective if the parent getting largest hubness value\n",
    "        # add a link that the node pointing to the node 1\n",
    "        temp = self.hubness.copy()\n",
    "        for _ in range(number):\n",
    "            node_to_be_added = max(temp.items(), key=operator.itemgetter(1))[0]\n",
    "            while '1' in self.graph[node_to_be_added]:\n",
    "                temp.pop(node_to_be_added)\n",
    "                node_to_be_added = max(temp.items(), key=operator.itemgetter(1))[0]\n",
    "            if type(self.graph[node_to_be_added]) is str:\n",
    "                self.graph[node_to_be_added] = []\n",
    "            if node_to_be_added == '1':\n",
    "                temp.pop(node_to_be_added)\n",
    "                node_to_be_added = max(temp.items(), key=operator.itemgetter(1))[0]\n",
    "            if type(self.parent_['1']) is str:\n",
    "                self.parent_['1'] = []\n",
    "            self.graph[node_to_be_added].append('1')\n",
    "            self.parent_['1'].append(node_to_be_added)\n",
    "        \n",
    "    def incease_hubness(self, number = 1):\n",
    "        # to increase hubness\n",
    "        # increase children\n",
    "        # effective if the children getting largest authority value\n",
    "        # add a link that the node 1 pointing to the another node\n",
    "        temp = self.authority.copy()\n",
    "        for _ in range(number):\n",
    "            node_to_add = max(temp.items(), key=operator.itemgetter(1))[0]\n",
    "            while node_to_add in self.graph['1']:\n",
    "                temp.pop(node_to_add)\n",
    "                node_to_add = max(temp.items(), key=operator.itemgetter(1))[0]\n",
    "            if node_to_add == '1':\n",
    "                temp.pop(node_to_add)\n",
    "                node_to_add = max(temp.items(), key=operator.itemgetter(1))[0]\n",
    "            if type(self.parent_['1']) is str:\n",
    "                self.parent_['1'] = []\n",
    "                \n",
    "\n",
    "            self.graph['1'].append(node_to_add)\n",
    "            self.parent_[node_to_add].append('1')\n",
    "            temp.pop(node_to_add)\n",
    "            \n",
    "    def find_parent(self):\n",
    "        self.parent_ = dict.fromkeys(self.graph, [])\n",
    "        for i in self.parent_:\n",
    "            par = []\n",
    "            for parent in self.graph:\n",
    "                if i in self.graph[parent]:\n",
    "                    par.append(parent)\n",
    "\n",
    "            self.parent_[i] = par.copy()\n",
    "        return self.parent_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     40
    ]
   },
   "outputs": [],
   "source": [
    "class SIMRANK:\n",
    "    def __init__(self, C, nodes, graph, iteration=20, damping_factor=0.15):\n",
    "        self.num_of_nodes = len(nodes)\n",
    "        self.C = C\n",
    "        self.nodes = nodes.copy()\n",
    "        self.graph = graph.copy()\n",
    "        self.damping_factor = damping_factor\n",
    "        self.iteration = iteration\n",
    "        self.parent_nodes, self.children_nodes = self.collect_in_out_nodes()\n",
    "        \n",
    "    def run(self):\n",
    "        self.sim_mat = np.zeros((self.num_of_nodes, self.num_of_nodes))\n",
    "        self.old_sim_mat = np.zeros(self.num_of_nodes)\n",
    "        for i in range(self.num_of_nodes):\n",
    "            self.sim_mat[i][i] = 1\n",
    "        \n",
    "        parent_ = dict.fromkeys(self.graph, [])\n",
    "\n",
    "        for i in parent_:\n",
    "            par = []\n",
    "            for parent in self.graph:\n",
    "                if i in self.graph[parent]:\n",
    "                    par.append(parent)\n",
    "            parent_[i] = par.copy()\n",
    "\n",
    "        for _iter in range(self.iteration):\n",
    "            cartesian_product = itertools.product(self.nodes, self.nodes)\n",
    "            self.old_sim_mat = self.sim_mat.copy()\n",
    "            for i1, i2 in cartesian_product:\n",
    "\n",
    "                if i1 is not i2 and len(parent_[i2]) !=0 and len(parent_[i1]) != 0: \n",
    "                    sum_of_item = 0\n",
    "                    for i in parent_[i1]:\n",
    "                        for j in parent_[i2]:\n",
    "                            sum_of_item += self.old_sim_mat[int(i)-1][int(int(j)-1)]\n",
    "                    self.sim_mat[int(i1)-1][int(i2)-1] = (self.C * sum_of_item) / (len(parent_[i2]) * len(parent_[i1]))\n",
    "        \n",
    "        return self.sim_mat\n",
    "        \n",
    "        \n",
    "    def collect_in_out_nodes(self):\n",
    "        parent_nodes = []\n",
    "        children_nodes = []\n",
    "        for parent in self.graph:\n",
    "            n = self.graph[parent]\n",
    "            if any(x is '0' for x in self.graph[parent]) is False:\n",
    "                parent_nodes.append(parent)\n",
    "            \n",
    "            for child in self.graph[parent]:\n",
    "                if int(child) != 0:\n",
    "                    if child not in children_nodes:\n",
    "                        children_nodes.append(child)\n",
    "        \n",
    "        return parent_nodes, children_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     24,
     62
    ]
   },
   "outputs": [],
   "source": [
    "class PAGERANK:\n",
    "    def __init__(self, nodes, graph, iteration=20, damping_factor=0.15):\n",
    "        self.num_of_nodes = len(nodes)\n",
    "        self.nodes = nodes.copy()\n",
    "        self.graph = graph.copy()\n",
    "        self.rank = dict.fromkeys(graph, 1.0 / self.num_of_nodes)\n",
    "        self.damping_factor = damping_factor\n",
    "        self.mininum_value = 1 - self.damping_factor\n",
    "        self.iteration = iteration\n",
    "        self.parent_nodes, self.children_nodes = self.collect_in_out_nodes()\n",
    "        self.find_parent()\n",
    "    # main function to run all actions   \n",
    "    def run(self):\n",
    "        for _iter in range(self.iteration):\n",
    "            for node in self.nodes:\n",
    "                self.rank[node] = self.damping_factor / self.num_of_nodes \n",
    "                for parent in self.parent_[node]:\n",
    "                        self.rank[node] +=  self.mininum_value * self.rank[parent] / len(self.graph[parent])\n",
    "#                         \n",
    "#             print('self.rank', self.rank)\n",
    "        return self.rank\n",
    "\n",
    "    # to collect whether the node is parent of children of other nodes\n",
    "    # just for jugdement of finding parent and children            \n",
    "    def collect_in_out_nodes(self):\n",
    "        parent_nodes = []\n",
    "        children_nodes = []\n",
    "        for parent in self.graph:\n",
    "            n = self.graph[parent]\n",
    "            if any(x is '0' for x in self.graph[parent]) is False:\n",
    "                parent_nodes.append(parent)\n",
    "            \n",
    "            for child in self.graph[parent]:\n",
    "                if int(child) != 0:\n",
    "                    if child not in children_nodes:\n",
    "                        children_nodes.append(child)\n",
    "        \n",
    "        return parent_nodes, children_nodes\n",
    "    \n",
    "    def increase_parent(self, number=1):\n",
    "        # to increase pagerank\n",
    "        # increase parent\n",
    "        # effective if the parent getting largest ranking value\n",
    "        # add a link that the node pointing to the node 1\n",
    "        temp = self.rank.copy()\n",
    "        for _ in range(number):\n",
    "            node_to_be_added = max(temp.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "            while '1' in self.graph[node_to_be_added]:\n",
    "                temp.pop(node_to_be_added)\n",
    "                node_to_be_added = max(temp.items(), key=operator.itemgetter(1))[0]\n",
    "                \n",
    "            if type(self.graph[node_to_be_added]) is str:\n",
    "                self.graph[node_to_be_added] = []\n",
    "\n",
    "            if node_to_be_added == '1':\n",
    "                temp.pop(node_to_be_added)\n",
    "                node_to_be_added = max(temp.items(), key=operator.itemgetter(1))[0]\n",
    "            \n",
    "            while '1' in self.graph[node_to_be_added]:\n",
    "                temp.pop(node_to_be_added)\n",
    "                node_to_be_added = max(temp.items(), key=operator.itemgetter(1))[0]\n",
    "                \n",
    "            if type(self.parent_['1']) is str:\n",
    "                self.parent_['1'] = []\n",
    "            \n",
    "            self.graph[node_to_be_added].append('1')\n",
    "            self.parent_['1'].append(node_to_be_added)\n",
    "        \n",
    "    def find_parent(self):\n",
    "        self.parent_ = dict.fromkeys(self.graph, [])\n",
    "        for i in self.parent_:\n",
    "            par = []\n",
    "            for parent in self.graph:\n",
    "                if i in self.graph[parent]:\n",
    "                    par.append(parent)\n",
    "\n",
    "            self.parent_[i] = par.copy()\n",
    "        return self.parent_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     7,
     36,
     40,
     45
    ]
   },
   "outputs": [],
   "source": [
    "def get_data_from_file(datapath = DATASET_PATH):\n",
    "    datafile = os.listdir(datapath)\n",
    "    for i in range(len(datafile)-1):\n",
    "        if datafile[i][0] == '.':\n",
    "            datafile.remove(datafile[i])\n",
    "    return datafile\n",
    "\n",
    "def read_file_to_graph(PATH, path):\n",
    "    nodes = []\n",
    "    graph = {}\n",
    "    with open(PATH + path) as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            node_parent, node_child = line.strip().split(',')\n",
    "            if node_parent not in graph:\n",
    "                nodes.append(node_parent)\n",
    "                graph[node_parent]=[]\n",
    "                \n",
    "            if node_child not in graph:\n",
    "                nodes.append(node_child)\n",
    "                graph[node_child]=[]\n",
    "\n",
    "            graph[node_parent].append(node_child)\n",
    "            \n",
    "            line = f.readline()\n",
    "            \n",
    "    for node in graph:\n",
    "        for child in graph[node]:\n",
    "            if child not in nodes:\n",
    "                nodes.append(child)\n",
    "    \n",
    "    for node in graph:\n",
    "        if len(graph[node])==0:\n",
    "            graph[node] = '0'\n",
    "    return nodes, graph\n",
    "\n",
    "def write_dataframe_to_file(name, authority, hubness, ranks, outputpath=RESULT_PATH):\n",
    "    name = name[:-4]+'_au_hub_rank.csv'\n",
    "    authority = collections.OrderedDict(sorted(authority.items()))\n",
    "    df = pd.DataFrame(columns=['Node', 'Authority', 'Hubness', 'Ranks'])\n",
    "    for i in authority:\n",
    "        df = df.append({'Node': i, 'Authority': authority[i], 'Hubness': hubness[i], 'Ranks': ranks[i]}, ignore_index=True)\n",
    "    df.to_csv(outputpath+name, sep='\\t', encoding='utf-8')\n",
    "    print('result dumped into ',outputpath+name)\n",
    "    \n",
    "def write_dataframe_to_file_v2(name, authority, authority_new, hubness, hubness_new, ranks, ranks_new, outputpath=RESULT_PATH):\n",
    "    name = name[:-4]+'_add_link.csv'\n",
    "    authority = collections.OrderedDict(sorted(authority.items()))\n",
    "    df = pd.DataFrame(columns=['Node', 'Authority', 'New Authority', 'Hubness', 'New Hubness', 'Ranks', 'New Ranks'])\n",
    "    for i in authority:\n",
    "        df = df.append({'Node': i, \n",
    "                        'Authority': authority[i], \n",
    "                        'New Authority': authority_new[i], \n",
    "                        'Hubness': hubness[i],\n",
    "                        'New Hubness': hubness_new[i], \n",
    "                        'Ranks': ranks[i], \n",
    "                        'New Ranks': ranks_new[i]}, ignore_index=True)\n",
    "    df.to_csv(outputpath+name, sep='\\t', encoding='utf-8')\n",
    "    print('result dumped into ',outputpath+name, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def ibm_data_convert(output_path, direction='directed', input_path='ibm/dataset.txt'):\n",
    "    outputpath = IBM_PATH + output_path\n",
    "    if os.path.exists(outputpath): \n",
    "        os.remove(outputpath) \n",
    "\n",
    "    with open(input_path, 'r') as rawfp:\n",
    "            for line in rawfp:\n",
    "                transaction = []\n",
    "                for item in line.split(','):\n",
    "                    transaction.append(item.strip())\n",
    "                \n",
    "                for i in range(len(transaction)):\n",
    "                    for j in range(i+1, len(transaction)):\n",
    "                        with open(outputpath, 'a') as convertedfp:\n",
    "                            convertedfp.write(str(transaction[i]+','+transaction[j]+'\\n'))\n",
    "                            if direction == 'bi-directed':\n",
    "                                convertedfp.write(str(transaction[j]+','+transaction[i]+'\\n'))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of HITS and PageRank\n",
    "PageRank with damping factor=0.15         \n",
    "\n",
    "calculate authority, hub and PageRank \n",
    "values for the following \n",
    "8 graphs \n",
    "* 6 graphs in project3dataset  \n",
    "* 1 graphs from project1 transaction data (connect items in each row, bi-directed or directed) \n",
    "\n",
    "iteration = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HITS and PageRank calculation of the 6 graphs in project3dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------HITS & PAGERANK with graph_6.txt --------------------\n",
      "time for HITS with  graph_6.txt to execute: 0.30824  second\n",
      "time for PAGERANK with  graph_6.txt to execute: 0.32058  second\n",
      "result dumped into  result/graph_6_au_hub_rank.csv\n",
      "\n",
      "----------HITS & PAGERANK with graph_5.txt --------------------\n",
      "time for HITS with  graph_5.txt to execute: 0.0342  second\n",
      "time for PAGERANK with  graph_5.txt to execute: 0.03756  second\n",
      "result dumped into  result/graph_5_au_hub_rank.csv\n",
      "\n",
      "----------HITS & PAGERANK with graph_1.txt --------------------\n",
      "time for HITS with  graph_1.txt to execute: 0.00016  second\n",
      "time for PAGERANK with  graph_1.txt to execute: 0.00021  second\n",
      "result dumped into  result/graph_1_au_hub_rank.csv\n",
      "-----Increasing hubness, authority, and PageRank of Node 1-----\n",
      "Authority BEFORE 0.0\n",
      "Authority  AFTER 0.4997559785261103\n",
      "Hubness   BEFORE 0.2\n",
      "Hubness    AFTER 0.6178646451730752\n",
      "PageRank  BEFORE 0.024999999999999998\n",
      "PageRank   AFTER 0.16666666618633766\n",
      "result dumped into  result/graph_1_add_link.csv \n",
      "\n",
      "\n",
      "----------HITS & PAGERANK with graph_4.txt --------------------\n",
      "time for HITS with  graph_4.txt to execute: 0.00016  second\n",
      "time for PAGERANK with  graph_4.txt to execute: 0.0002  second\n",
      "result dumped into  result/graph_4_au_hub_rank.csv\n",
      "\n",
      "----------HITS & PAGERANK with graph_3.txt --------------------\n",
      "time for HITS with  graph_3.txt to execute: 7e-05  second\n",
      "time for PAGERANK with  graph_3.txt to execute: 0.0001  second\n",
      "result dumped into  result/graph_3_au_hub_rank.csv\n",
      "-----Increasing hubness, authority, and PageRank of Node 1-----\n",
      "Authority BEFORE 0.19090909090909092\n",
      "Authority  AFTER 0.3383098192765027\n",
      "Hubness   BEFORE 0.19101123595505617\n",
      "Hubness    AFTER 0.33830424122300395\n",
      "PageRank  BEFORE 0.17545159217677955\n",
      "PageRank   AFTER 0.26166105722423416\n",
      "result dumped into  result/graph_3_add_link.csv \n",
      "\n",
      "\n",
      "----------HITS & PAGERANK with graph_2.txt --------------------\n",
      "time for HITS with  graph_2.txt to execute: 5e-05  second\n",
      "time for PAGERANK with  graph_2.txt to execute: 8e-05  second\n",
      "result dumped into  result/graph_2_au_hub_rank.csv\n",
      "-----Increasing hubness, authority, and PageRank of Node 1-----\n",
      "Authority BEFORE 0.2\n",
      "Authority  AFTER 0.6177600472813239\n",
      "Hubness   BEFORE 0.2\n",
      "Hubness    AFTER 0.6178646451730752\n",
      "PageRank  BEFORE 0.2\n",
      "PageRank   AFTER 0.22465469053484255\n",
      "result dumped into  result/graph_2_add_link.csv \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## # for the 6 graphs in project3dataset  \n",
    "datafile = get_data_from_file()\n",
    "first_3_graph = ['graph_1.txt', 'graph_2.txt', 'graph_3.txt']\n",
    "for file in datafile:\n",
    "    nodes, graph = read_file_to_graph(DATASET_PATH, file)\n",
    "\n",
    "    print('\\n----------HITS & PAGERANK with', file, '--------------------')\n",
    "    start_time = time.time()\n",
    "    hits1 = HITS(nodes, graph)\n",
    "    authority, hubness = hits1.run()\n",
    "    end_time = time.time()\n",
    "    print('time for HITS with ', file,'to execute:', round(end_time- start_time, 5), ' second')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    pagerank = PAGERANK(nodes, graph)\n",
    "    rank = pagerank.run().copy()\n",
    "    end_time = time.time()\n",
    "    print('time for PAGERANK with ', file,'to execute:', round(end_time- start_time, 5), ' second')\n",
    "\n",
    "    write_dataframe_to_file(file, authority, hubness, rank)\n",
    "    \n",
    "    if file in first_3_graph:\n",
    "        print('-----Increasing hubness, authority, and PageRank of Node 1-----') \n",
    "        nodes, graph = read_file_to_graph(DATASET_PATH, file)\n",
    "        hits2 = HITS(nodes, graph)\n",
    "        authority_na, hubness_na = hits2.run()\n",
    "        hits2.incease_authority()\n",
    "        authority_na, hubness_na = hits2.run()\n",
    "        \n",
    "        nodes, graph = read_file_to_graph(DATASET_PATH, file)\n",
    "        hits3 = HITS(nodes, graph)\n",
    "        authority_nh, hubness_nh = hits3.run()\n",
    "        hits3.incease_hubness()\n",
    "        authority_nh, hubness_nh = hits3.run()\n",
    "        print('Authority BEFORE', authority['1'])\n",
    "        print('Authority  AFTER', authority_na['1'])\n",
    "        print('Hubness   BEFORE', hubness['1'])\n",
    "        print('Hubness    AFTER', hubness_nh['1'])\n",
    "\n",
    "        pagerank.increase_parent()\n",
    "        rank_new = pagerank.run()\n",
    "        print('PageRank  BEFORE', rank['1'])\n",
    "        print('PageRank   AFTER', rank_new['1'])\n",
    "\n",
    "        write_dataframe_to_file_v2(file, authority, authority_na, hubness, hubness_nh, rank, rank_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HITS and PageRank calculation of the graphs from project1 transaction data \n",
    "(connect items in each row, bi-directed or directed)\n",
    "```\n",
    "['graph_1.txt', 'graph_2.txt', 'graph_3.txt', 'graph_4.txt', 'graph_5.txt']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation of the transaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_data_convert('directed_ibm.txt', direction='directed', input_path='ibm/dataset.txt')\n",
    "ibm_data_convert('bidirected_ibm.txt', direction='bi-directed', input_path='ibm/dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------HITS & PAGERANK with directed_ibm.txt --------------------\n",
      "time for HITS with  directed_ibm.txt to execute: 0.23326  second\n",
      "time for PAGERANK with  directed_ibm.txt to execute: 0.27354  second\n",
      "result dumped into  result/directed_ibm_au_hub_rank.csv\n",
      "\n",
      "----------HITS & PAGERANK with bidirected_ibm.txt --------------------\n",
      "time for HITS with  bidirected_ibm.txt to execute: 0.50474  second\n",
      "time for PAGERANK with  bidirected_ibm.txt to execute: 0.56268  second\n",
      "result dumped into  result/bidirected_ibm_au_hub_rank.csv\n"
     ]
    }
   ],
   "source": [
    "converted_ibm_graph = ['directed_ibm.txt', 'bidirected_ibm.txt']\n",
    "for file in converted_ibm_graph:\n",
    "    nodes, graph = read_file_to_graph(IBM_PATH, file)\n",
    "    \n",
    "    print('\\n----------HITS & PAGERANK with', file, '--------------------')\n",
    "    start_time = time.time()\n",
    "    hits1 = HITS(nodes, graph)\n",
    "    authority, hubness = hits1.run()\n",
    "    end_time = time.time()\n",
    "    print('time for HITS with ', file,'to execute:', round(end_time- start_time, 5), ' second')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    pagerank = PAGERANK(nodes, graph)\n",
    "    rank = pagerank.run()\n",
    "    end_time = time.time()\n",
    "    print('time for PAGERANK with ', file,'to execute:', round(end_time- start_time, 5), ' second')\n",
    "    \n",
    "    write_dataframe_to_file(file, authority, hubness, rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMRANK\n",
    "to calculate pair-wise similarity of nodes of the first 5 graphs of project3dataset\n",
    "\n",
    "C= 0.9\n",
    "iteration=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for graph_1.txt to execute: 0.0027  second\n",
      "time for graph_2.txt to execute: 0.00244  second\n",
      "time for graph_3.txt to execute: 0.00229  second\n",
      "time for graph_4.txt to execute: 0.00984  second\n",
      "time for graph_5.txt to execute: 30.33711  second\n"
     ]
    }
   ],
   "source": [
    "first_5_datafile = []\n",
    "for i in range(5):\n",
    "    j = i + 1\n",
    "    for file in datafile:\n",
    "        if file[6]==str(j):\n",
    "            first_5_datafile.append(file)\n",
    "            \n",
    "for file in first_5_datafile:\n",
    "    start_time = time.time()\n",
    "    nodes, graph = read_file_to_graph(DATASET_PATH, file)\n",
    "    simrank = SIMRANK(0.9, nodes, graph)\n",
    "    sim = simrank.run()\n",
    "    np.savetxt(RESULT_PATH+file[:-4]+'_simrank.txt', sim, delimiter=',')\n",
    "    end_time = time.time()\n",
    "    print('time for', file,'to execute:', round(end_time- start_time, 5), ' second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
